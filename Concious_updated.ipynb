{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_login"
      },
      "source": [
        "### Block 0: Hugging Face Login\n",
        "This block logs you into your Hugging Face account, which is necessary to download and use certain models from the Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsND-dS97faa"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "print(\"--- Please log in to your Hugging Face account. ---\")\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_install"
      },
      "source": [
        "### Block 1: Installations\n",
        "This block ensures all necessary Python libraries for the project are installed. This includes libraries for deep learning (`transformers`, `torch`), vector databases (`faiss-cpu`), natural language processing (`langchain`), and the user interface (`gradio`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tkr6CZh5ELt"
      },
      "outputs": [],
      "source": [
        "print(\"--- Installing required packages... ---\")\n",
        "!pip install langchain langchain-community faiss-cpu transformers sentence-transformers gradio pandas torch plotly accelerate bitsandbytes -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_imports"
      },
      "source": [
        "### Block 2: Imports & Global Setup\n",
        "Here we import all the required modules and define global constants that will be used throughout the notebook. This includes file paths, model names, and classification labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bze721tS5KdY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from datetime import datetime, timedelta\n",
        "import torch\n",
        "import re\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline, logging, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "DB_PATH = \"conscious_memory\"\n",
        "JOURNAL_PATH = \"journal_log.csv\"\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "LLM_MODEL = \"google/flan-t5-large\"\n",
        "CLASSIFIER_MODEL = \"facebook/bart-large-mnli\"\n",
        "\n",
        "EMOTION_LABELS = [\"happy\", \"sad\", \"angry\", \"hopeful\", \"guilty\", \"calm\", \"anxious\", \"confused\"]\n",
        "VALUE_LABELS = [\"honesty\", \"trust\", \"communication\", \"regret\", \"growth\", \"forgiveness\", \"courage\"]\n",
        "\n",
        "print(\"‚úÖ Libraries imported and variables set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_brain"
      },
      "source": [
        "### Block 3: Core AI Logic (The \"Brain\")\n",
        "These functions define the core reasoning and response generation capabilities of the AI. This includes both static (fallback) and dynamic (context-aware) response generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rsa85-FU6W83"
      },
      "outputs": [],
      "source": [
        "def generate_static_response(emotion):\n",
        "    \"\"\"\n",
        "    Provides a fallback response when not enough data exists to generate a dynamic one.\n",
        "    \"\"\"\n",
        "    responses = {\n",
        "        \"happy\": \"I'm glad to hear that! üòä Want to reflect more on what made you feel this way?\",\n",
        "        \"sad\": \"I'm here with you. üòî Want to talk more about what‚Äôs been bothering you?\",\n",
        "        \"angry\": \"That sounds intense. üò§ Want to unpack why you feel this way?\",\n",
        "        \"guilty\": \"Guilt can be heavy. üßò What could help you forgive yourself?\",\n",
        "        \"hopeful\": \"That‚Äôs a beautiful mindset. üå± Want to explore what‚Äôs fueling that hope?\",\n",
        "        \"confused\": \"It sounds like you're trying to make sense of things. ü§î Want to clarify your thoughts together?\",\n",
        "        \"calm\": \"Peaceful vibes. üßò Want to write about what‚Äôs helping you stay grounded?\"\n",
        "    }\n",
        "    return responses.get(emotion.lower(), \"I'm listening. Feel free to share more. ü´∂\")\n",
        "\n",
        "def generate_dynamic_response(emotion, current_text, llm, db, persona=\"Supportive\"):\n",
        "    \"\"\"\n",
        "    Generates an advanced, personalized response by learning from past helpful feedback \n",
        "    and semantically similar entries from the vector database.\n",
        "    \"\"\"\n",
        "    print(\"üß† Generating a dynamic response...\")\n",
        "    \n",
        "    persona_descriptions = {\n",
        "        \"Supportive\": \"a supportive, deeply empathetic, and insightful journal companion.\",\n",
        "        \"Therapist-like\": \"a reflective, non-judgmental, and insightful therapist-like journal companion.\",\n",
        "        \"Coach\": \"an encouraging, action-oriented, and insightful coach-like journal companion.\",\n",
        "        \"Neutral\": \"a concise and factual journal companion.\",\n",
        "    }\n",
        "    ai_persona_description = persona_descriptions.get(persona, persona_descriptions[\"Supportive\"])\n",
        "\n",
        "    if not os.path.exists(JOURNAL_PATH):\n",
        "        return generate_static_response(emotion)\n",
        "\n",
        "    df = pd.read_csv(JOURNAL_PATH)\n",
        "    good_examples = df[(df['emotion'] == emotion) & (df['feedback'] == 'Insightful')]\n",
        "\n",
        "    example_text = \"\"\n",
        "    if not good_examples.empty:\n",
        "        examples_to_use = good_examples.tail(5)\n",
        "        for _, row in examples_to_use.iterrows():\n",
        "            example_text += f\"User Entry: \\\"{row['text']}\\\"\\nHelpful Response: \\\"{row['ai_response']}\\\"\\n\\n\"\n",
        "    \n",
        "    context_from_db = \"\"\n",
        "    if db is not None:\n",
        "        try:\n",
        "            retrieved_docs = db.similarity_search(current_text, k=3)\n",
        "            if retrieved_docs:\n",
        "                context_from_db = \"\\nHere are some of your past reflections that resonate with your current thoughts:\\n---\\n\"\n",
        "                for i, doc in enumerate(retrieved_docs):\n",
        "                    context_from_db += f\"Entry on {doc.metadata.get('timestamp', 'N/A')}: \\\"{doc.page_content}\\\"\\n\"\n",
        "                context_from_db += \"---\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå ERROR retrieving from FAISS: {e}\")\n",
        "\n",
        "    prompt = f\"\"\"You are {ai_persona_description}\n",
        "Your purpose is to help the user reflect deeper. Your responses must be thoughtful, unique, and directly relevant.\n",
        "\n",
        "Here are examples of past responses the user found helpful for '{emotion}':\n",
        "{example_text}\n",
        "{context_from_db}\n",
        "\n",
        "Read the user's new entry and generate one new, thoughtful response.\n",
        "\n",
        "**Rules:**\n",
        "1. Start with ONE specific, empathetic sentence that validates a core feeling from the user's entry.\n",
        "2. End with ONE gentle, open-ended question to encourage deeper reflection.\n",
        "3. Do NOT give advice, use the word \"I\", or add extra conversational filler.\n",
        "4. Do NOT use generic phrases like \"You're right\" or \"That's a good point.\"\n",
        "\n",
        "**New User Entry:** \"{current_text}\"\n",
        "\n",
        "**Your Response (validation sentence and question only):**\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        raw_response = llm.invoke(prompt)\n",
        "        response_text = str(raw_response).strip()\n",
        "        \n",
        "        # Clean the response from any echoed prompt text\n",
        "        response_text = response_text.replace(\"Your Response (validation sentence and question only):\", \"\").strip()\n",
        "        \n",
        "        sentences = [s.strip() for s in re.split(r'[.?!]', response_text) if s.strip()]\n",
        "        \n",
        "        if sentences:\n",
        "            validation_sentence = sentences[0]\n",
        "            question_sentence = next((s for s in reversed(sentences) if s.endswith('?')), \"What is on your mind most about this?\")\n",
        "            \n",
        "            if not validation_sentence.endswith(('.', '?', '!')): validation_sentence += '.'\n",
        "            if not question_sentence.endswith('?'): question_sentence += '?'\n",
        "            \n",
        "            clean_response = f\"{validation_sentence.strip()} {question_sentence.strip()}\"\n",
        "        else:\n",
        "            clean_response = \"It sounds like you have a lot on your mind. Would you like to explore that further?\"\n",
        "            \n",
        "        return clean_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR during LLM call: {e}\")\n",
        "        return f\"An error occurred while generating a response: {e}\"\n",
        "\n",
        "print(\"‚úÖ Core AI logic defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_data"
      },
      "source": [
        "### Block 4: Data & Memory Functions\n",
        "These helper functions handle the data persistence layer. They manage saving and loading journal entries to a CSV file, handling user feedback, calculating streaks, analyzing journal data for visualizations, and querying the journal history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtN_MUnT57iR"
      },
      "outputs": [],
      "source": [
        "def save_journal_to_csv(timestamp, text, emotion, value, ai_response):\n",
        "    \"\"\"Saves a record of the conversation to a CSV file.\"\"\"\n",
        "    new_entry = {\"timestamp\": timestamp, \"text\": text, \"emotion\": emotion, \"value_theme\": value, \"ai_response\": ai_response, \"feedback\": \"\"}\n",
        "    df_columns = list(new_entry.keys())\n",
        "    \n",
        "    if os.path.exists(JOURNAL_PATH):\n",
        "        df = pd.DataFrame([new_entry])\n",
        "        df.to_csv(JOURNAL_PATH, mode='a', header=False, index=False)\n",
        "    else:\n",
        "        df = pd.DataFrame([new_entry], columns=df_columns)\n",
        "        df.to_csv(JOURNAL_PATH, index=False)\n",
        "\n",
        "def add_journal_entry(text, classifier, db, embedding, llm, persona_selection=\"Supportive\"):\n",
        "    \"\"\"Main function to process and save a new journal entry.\"\"\"\n",
        "    if not text:\n",
        "        return \"\", \"\", \"\", None\n",
        "\n",
        "    emotion_result = classifier(text, EMOTION_LABELS)\n",
        "    value_result = classifier(text, VALUE_LABELS)\n",
        "    top_emotion, top_value = emotion_result['labels'][0], value_result['labels'][0]\n",
        "    timestamp = datetime.now().isoformat()\n",
        "\n",
        "    ai_response = generate_dynamic_response(top_emotion, text, llm, db, persona_selection)\n",
        "\n",
        "    save_journal_to_csv(timestamp, text, top_emotion, top_value, ai_response)\n",
        "    metadata = {\"timestamp\": timestamp, \"emotion\": top_emotion, \"value_theme\": top_value}\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    new_docs = splitter.split_documents([Document(page_content=text, metadata=metadata)])\n",
        "    db.add_documents(new_docs)\n",
        "    db.save_local(DB_PATH)\n",
        "\n",
        "    print(f\"‚úÖ Entry added! Emotion: {top_emotion} | Value: {top_value}\")\n",
        "    return ai_response, top_emotion, top_value, timestamp\n",
        "\n",
        "def handle_feedback(feedback_type, entry_timestamp):\n",
        "    \"\"\"Saves the user's feedback for a specific entry to the CSV.\"\"\"\n",
        "    if not entry_timestamp or not feedback_type: return \"Please submit an entry and select a feedback option first.\"\n",
        "    \n",
        "    if os.path.exists(JOURNAL_PATH):\n",
        "        df = pd.read_csv(JOURNAL_PATH, dtype={'timestamp': str})\n",
        "        df.loc[df['timestamp'] == entry_timestamp, 'feedback'] = feedback_type\n",
        "        df.to_csv(JOURNAL_PATH, index=False)\n",
        "        return \"Thank you for the feedback! I'm learning. üß†\"\n",
        "    return \"Journal file not found.\"\n",
        "\n",
        "def calculate_streak():\n",
        "    \"\"\"Calculates the current daily journaling streak.\"\"\"\n",
        "    if not os.path.exists(JOURNAL_PATH): return 0, \"\"\n",
        "    df = pd.read_csv(JOURNAL_PATH)\n",
        "    if df.empty: return 0, \"\"\n",
        "\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    unique_dates = sorted(df['timestamp'].dt.date.unique())\n",
        "    if not unique_dates: return 0, \"\"\n",
        "\n",
        "    today = datetime.now().date()\n",
        "    if today not in unique_dates and (today - timedelta(days=1)) not in unique_dates:\n",
        "        return 0, \"Start your streak today!\"\n",
        "\n",
        "    current_streak = 0\n",
        "    check_date = today\n",
        "    while check_date in unique_dates:\n",
        "        current_streak += 1\n",
        "        check_date -= timedelta(days=1)\n",
        "        \n",
        "    message = f\"üî• Day {current_streak} streak! Keep reflecting.\" if current_streak > 0 else \"Start your streak today!\"\n",
        "    return current_streak, message\n",
        "\n",
        "def analyze_journal():\n",
        "    \"\"\"Generates plots for the Analytics tab.\"\"\"\n",
        "    if not os.path.exists(JOURNAL_PATH) or pd.read_csv(JOURNAL_PATH).empty:\n",
        "        empty_fig = go.Figure()\n",
        "        return \"No journal data to analyze yet.\", empty_fig, empty_fig, empty_fig, empty_fig\n",
        "\n",
        "    df = pd.read_csv(JOURNAL_PATH)\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['date'] = df['timestamp'].dt.date\n",
        "\n",
        "    emotion_counts = df['emotion'].value_counts().reset_index()\n",
        "    fig1 = px.bar(emotion_counts, x='emotion', y='count', title='Emotion Frequency', color='emotion')\n",
        "\n",
        "    value_counts = df['value_theme'].value_counts().reset_index()\n",
        "    fig2 = px.pie(value_counts, names='value_theme', values='count', title='Core Value Themes', hole=.3)\n",
        "\n",
        "    emotion_daily_counts = df.groupby(['date', 'emotion']).size().reset_index(name='count')\n",
        "    fig3 = px.line(emotion_daily_counts, x='date', y='count', color='emotion', title='Emotion Trends Over Time', markers=True)\n",
        "\n",
        "    value_daily_counts = df.groupby(['date', 'value_theme']).size().reset_index(name='count')\n",
        "    fig4 = px.line(value_daily_counts, x='date', y='count', color='value_theme', title='Core Value Themes Over Time', markers=True)\n",
        "\n",
        "    return \"‚úÖ Analysis Complete!\", fig1, fig2, fig3, fig4\n",
        "\n",
        "def generate_weekly_summary(llm):\n",
        "    \"\"\"Generates a summary of journal entries from the last 7 days.\"\"\"\n",
        "    if not os.path.exists(JOURNAL_PATH) or pd.read_csv(JOURNAL_PATH).empty:\n",
        "        return \"No journal data available for summary yet.\"\n",
        "\n",
        "    df = pd.read_csv(JOURNAL_PATH)\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    seven_days_ago = datetime.now() - timedelta(days=7)\n",
        "    recent_entries_df = df[df['timestamp'] >= seven_days_ago]\n",
        "\n",
        "    if recent_entries_df.empty: return \"No entries in the last 7 days to summarize.\"\n",
        "\n",
        "    summary_text_entries = \"\".join([f\"Entry on {row['timestamp'].date()}: \\\"{row['text']}\\\"\\n\" for _, row in recent_entries_df.iterrows()])\n",
        "    \n",
        "    summary_prompt = f\"\"\"You are an insightful summarization agent. Based on the following journal entries from the past week, provide a concise, 2-3 sentence summary covering the overall emotional tone and any recurring themes. Conclude with a gentle thought or question.\n",
        "\n",
        "    Entries:\n",
        "    {summary_text_entries}\n",
        "\n",
        "    Your Summary:\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return str(llm.invoke(summary_prompt)).strip()\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred while generating the summary: {e}\"\n",
        "\n",
        "def ask_journal(query, llm, db):\n",
        "    \"\"\"Allows the user to ask questions about their journal history.\"\"\"\n",
        "    if not query: return \"Please type a question to ask your journal.\"\n",
        "    \n",
        "    try:\n",
        "        retrieved_docs = db.similarity_search(query, k=5)\n",
        "        if not retrieved_docs: return \"I couldn't find any relevant entries for that question.\"\n",
        "\n",
        "        context = \"\".join([f\"Entry on {doc.metadata.get('timestamp', 'N/A')}: {doc.page_content}\\n\" for doc in retrieved_docs])\n",
        "        \n",
        "        prompt = f\"\"\"You are an assistant that summarizes journal entries. Based on the following entries, answer the user's question concisely. If the information isn't present, say so.\n",
        "\n",
        "        Entries:\n",
        "        {context}\n",
        "\n",
        "        User's Question: \"{query}\"\n",
        "\n",
        "        Concise Answer:\n",
        "        \"\"\"\n",
        "        return str(llm.invoke(prompt)).strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "def download_journal_csv():\n",
        "    \"\"\"Lets the user download their complete journal as a CSV file.\"\"\"\n",
        "    if not os.path.exists(JOURNAL_PATH):\n",
        "        pd.DataFrame(columns=[\"timestamp\", \"text\", \"emotion\", \"value_theme\", \"ai_response\", \"feedback\"]).to_csv(JOURNAL_PATH, index=False)\n",
        "    return JOURNAL_PATH\n",
        "\n",
        "print(\"‚úÖ Data and memory functions defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_main"
      },
      "source": [
        "### Block 5: Main Application Execution\n",
        "This is the main block that initializes all the AI models and the vector database, then builds and launches the Gradio user interface. It defines wrapper functions to handle model scope within the Gradio app and connects all the UI components to their corresponding backend logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6vCE6Co5vEs"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- Initializing AI Models and Vector Database... ---\")\n",
        "\n",
        "    print(\"1. Loading embedding model...\")\n",
        "    embedding = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
        "\n",
        "    print(\"2. Setting up vector database (FAISS)...\")\n",
        "    if os.path.exists(DB_PATH):\n",
        "        db = FAISS.load_local(DB_PATH, embedding, allow_dangerous_deserialization=True)\n",
        "    else:\n",
        "        db = FAISS.from_documents([Document(page_content=\"Welcome to your journal.\")], embedding)\n",
        "        db.save_local(DB_PATH)\n",
        "\n",
        "    print(\"3. Loading classifier model...\")\n",
        "    classifier = pipeline(\"zero-shot-classification\", model=CLASSIFIER_MODEL)\n",
        "\n",
        "    print(\"4. Loading primary language model (LLM)...\")\n",
        "    quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL, quantization_config=quantization_config, device_map=\"auto\")\n",
        "    \n",
        "    llm_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256, temperature=0.7, top_p=0.95)\n",
        "    llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
        "\n",
        "    print(\"‚úÖ All models initialized successfully!\")\n",
        "\n",
        "    def add_journal_entry_wrapper(text, persona):\n",
        "        return add_journal_entry(text, classifier, db, embedding, llm, persona)\n",
        "\n",
        "    def generate_weekly_summary_wrapper():\n",
        "        return generate_weekly_summary(llm)\n",
        "\n",
        "    def ask_journal_wrapper(query):\n",
        "        return ask_journal(query, llm, db)\n",
        "\n",
        "    print(\"--- Building the User Interface... ---\")\n",
        "    with gr.Blocks(theme=gr.themes.Soft(), title=\"ConsciousAI Journal\") as demo:\n",
        "        gr.Markdown(\"# üß† ConsciousAI Journal\")\n",
        "        gr.Markdown(\"A safe space for self-reflection. Write about your day, and I'll help you understand your feelings and values.\")\n",
        "\n",
        "        last_entry_timestamp = gr.State()\n",
        "        streak_display = gr.Textbox(label=\"Journaling Streak\", interactive=False, value=calculate_streak()[1])\n",
        "        ai_persona_selection = gr.Dropdown([\"Supportive\", \"Therapist-like\", \"Coach\", \"Neutral\"], label=\"Choose AI Persona:\", value=\"Supportive\", interactive=True)\n",
        "\n",
        "        with gr.Tabs():\n",
        "            with gr.TabItem(\"‚úçÔ∏è Journal\"):\n",
        "                text_input = gr.Textbox(label=\"What's on your mind?\", lines=10, placeholder=\"Today I felt...\")\n",
        "                with gr.Row():\n",
        "                    submit_btn = gr.Button(\"Submit Entry\", variant=\"primary\")\n",
        "                    clear_btn = gr.Button(\"Clear\")\n",
        "                response_output = gr.Textbox(label=\"AI Companion's Response\", interactive=False, lines=4)\n",
        "                gr.Markdown(\"How did that response feel?\")\n",
        "                feedback_options = gr.Radio([\"Insightful\", \"Made me think\", \"A bit generic\", \"Didn't feel right\"], label=\"Choose what best describes the AI's response:\", value=None)\n",
        "                submit_feedback_btn = gr.Button(\"Submit Feedback\", variant=\"secondary\")\n",
        "                feedback_status = gr.Textbox(label=\"Feedback Status\", interactive=False)\n",
        "                with gr.Row():\n",
        "                    emotion_output = gr.Textbox(label=\"Detected Emotion\", interactive=False)\n",
        "                    value_output = gr.Textbox(label=\"Detected Core Value\", interactive=False)\n",
        "                with gr.Row():\n",
        "                    download_btn = gr.Button(\"üì• Download Journal CSV\")\n",
        "                    csv_output = gr.File(label=\"Your journal data\")\n",
        "\n",
        "            with gr.TabItem(\"üìä Analytics\"):\n",
        "                analyze_btn = gr.Button(\"Analyze My Journal\", variant=\"primary\")\n",
        "                analysis_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "                with gr.Row():\n",
        "                    plot_emotion_freq = gr.Plot()\n",
        "                    plot_value_pie = gr.Plot()\n",
        "                with gr.Row():\n",
        "                    plot_emotion_trend = gr.Plot(label=\"Emotion Trend\")\n",
        "                    plot_value_trend = gr.Plot(label=\"Core Value Trend\")\n",
        "                summary_btn = gr.Button(\"Generate Weekly Summary\", variant=\"secondary\")\n",
        "                weekly_summary_output = gr.Textbox(label=\"Your Weekly Reflection\", interactive=False, lines=5)\n",
        "\n",
        "            with gr.TabItem(\"üß≠ Ask Your Journal\"):\n",
        "                query_input = gr.Textbox(label=\"Your Question\", lines=2, placeholder=\"When did I last feel calm?\")\n",
        "                ask_btn = gr.Button(\"Ask Journal\", variant=\"primary\")\n",
        "                journal_answer_output = gr.Textbox(label=\"Journal's Answer\", lines=5, interactive=False)\n",
        "\n",
        "        submit_btn.click(fn=add_journal_entry_wrapper, inputs=[text_input, ai_persona_selection], outputs=[response_output, emotion_output, value_output, last_entry_timestamp]).then(fn=lambda: calculate_streak()[1], outputs=streak_display)\n",
        "        clear_btn.click(lambda: [\"\", \"\", \"\", None, None, None], outputs=[text_input, response_output, emotion_output, value_output, feedback_status, feedback_options])\n",
        "        download_btn.click(fn=download_journal_csv, outputs=csv_output)\n",
        "        submit_feedback_btn.click(fn=handle_feedback, inputs=[feedback_options, last_entry_timestamp], outputs=feedback_status)\n",
        "        analyze_btn.click(fn=analyze_journal, outputs=[analysis_status, plot_emotion_freq, plot_value_pie, plot_emotion_trend, plot_value_trend])\n",
        "        summary_btn.click(fn=generate_weekly_summary_wrapper, outputs=weekly_summary_output)\n",
        "        ask_btn.click(fn=ask_journal_wrapper, inputs=query_input, outputs=journal_answer_output)\n",
        "\n",
        "    print(\"üöÄ Launching Gradio App...\")\n",
        "    demo.launch(debug=True, share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
